# Docker Compose config for mltoolbox
# This file lives in .mlt/ - build context points to parent (project root)
services:
  {{ container_name | lower }}:
    env_file:
      - ../.env  # .env stays in project root for easy access
    build:
      context: ..  # Build context is project root (parent of .mlt/)
      dockerfile: .mlt/Dockerfile  # Dockerfile path relative to context
      args:
        PYTHON_VERSION: ${PYTHON_VERSION}
        GIT_NAME: ${GIT_NAME}
        GIT_EMAIL: ${GIT_EMAIL}
        PROJECT_NAME: ${PROJECT_NAME}
        VARIANT: ${VARIANT}
        DEPENDENCY_TAGS: ${DEPENDENCY_TAGS:-dev}
    image: ${CONTAINER_NAME}:latest
    container_name: ${CONTAINER_NAME}
    hostname: ${CONTAINER_NAME}
    working_dir: /workspace/${PROJECT_NAME}
    network_mode: host  # Use host networking
    volumes:
      - ..:/workspace/${PROJECT_NAME}  # Mount project root (parent of .mlt/)
      - ~/.config/rclone:/root/.config/rclone
      - ~/.claude:/root/.claude
      - /var/run/docker.sock:/var/run/docker.sock
      - /tmp/ray_lockfiles:/root/ray_lockfiles
      - type: bind
        source: ~/.ssh
        target: /root/.ssh
    environment:
      - CONTAINER_ID=${CONTAINER_NAME}
      - RAY_HEAD_ADDRESS=localhost:6379
      # Container SSH port (always enabled, configurable port)
      - CONTAINER_SSH_PORT=${CONTAINER_SSH_PORT:-2222}
      # Jupyter configuration
      - ENABLE_JUPYTER=${ENABLE_JUPYTER:-false}
      - JUPYTER_PORT=${JUPYTER_PORT:-8888}
      # No need for port mapping with host networking
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    stdin_open: true
    tty: true
    restart: unless-stopped
